# ä»…é¦–æ¬¡è¿è¡Œ
!pip install gradio
!pip install transformers torchaudio audiocraft accelerate bitsandbytes

#2 qwen-musicgen-meduim API 
import json
import random
import requests
import gradio as gr

# Qwen API é…ç½®
QWEN_API_KEY = "sk-"  # é˜¿é‡Œäº‘ Key
QWEN_API_URL = "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation"

# Hugging Face MusicGen API
HF_API_TOKEN = ""  # Hugging Face Token
HF_API_URL = "https://api-inference.huggingface.co/models/facebook/musicgen-medium"
HF_HEADERS = {"Authorization": f"Bearer {HF_API_TOKEN}"}

# ä½¿ç”¨é…é¢è®¾ç½®
DAILY_LIMIT = 10
call_counter = {"count": 0}

# è¯»å–æ•°æ® {"scene": ..., "music": ...}
scene_music_data = []
with open("musicgen_scene_music_pairs.jsonl", "r", encoding="utf-8") as f:
    for line in f:
        scene_music_data.append(json.loads(line.strip()))

# ==========================few-shot===================================
# Step 1: few-shot messagesæ„å»º
def build_few_shot_messages(k=2):
    examples = random.sample(scene_music_data, k)
    messages = [{"role": "system", "content": "You are a music cognition expert converting restaurant scene descriptions into music prompts suitable for MusicGen."}]
    for ex in examples:
        messages.append({"role": "user", "content": f"Scene: {ex['scene']}"})
        messages.append({"role": "assistant", "content": ex['music']})
    return messages

# Step 2: Qwenç”ŸæˆéŸ³ä¹æè¿°
def generate_music_description(scene_desc):
    messages = build_few_shot_messages(k=2)
    messages.append({"role": "user", "content": f"Scene: {scene_desc}"})

    headers = {
        "Authorization": f"Bearer {QWEN_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": "qwen2.5-14b-instruct",
        "input": {"messages": messages},
        "parameters": {"temperature": 0.5, "max_tokens": 150}
    }
    response = requests.post(QWEN_API_URL, headers=headers, json=payload)
    if response.status_code == 200:
        result = response.json()
        return result.get("output", {}).get("text", "No output")
    else:
        return f"Error: {response.text}"

# ==========================éŸ³ä¹ç”Ÿæˆ===================================
# Step 3: è°ƒç”¨MusicGen API
def generate_music_from_text(music_desc):
    if call_counter["count"] >= DAILY_LIMIT:
        return "å·²è¾¾åˆ°ä»Šæ—¥è°ƒç”¨ä¸Šé™ï¼Œè¯·æ˜æ—¥å†è¯•æˆ–å‡çº§é…é¢", None
    response = requests.post(HF_API_URL, headers=HF_HEADERS, json={"inputs": music_desc})
    if response.status_code == 200:
        audio_path = "musicgen_output.wav"
        with open(audio_path, "wb") as f:
            f.write(response.content)
        call_counter["count"] += 1
        return audio_path
    else:
        return f"API Error: {response.status_code} - {response.text}", None

# Gradio æ¥å£
def gradio_generate(scene_description):
    music_desc = generate_music_description(scene_description)
    if "Error" in music_desc or "è°ƒç”¨ä¸Šé™" in music_desc:
        return music_desc, None
    result = generate_music_from_text(music_desc)
    if isinstance(result, tuple):
        return result[0], None
    return music_desc, result

# Gradio é¡µé¢æ„å»º
with gr.Blocks() as demo:
    gr.Markdown("## èƒŒæ™¯éŸ³ä¹ç”Ÿæˆå™¨")
    scene_input = gr.Textbox(label="é¤é¦†ç¯å¢ƒæè¿°", placeholder="å¦‚ï¼šæ¸©é¦¨çš„æ„å¤§åˆ©é¤å…ï¼Œé€‚åˆæƒ…ä¾£çº¦ä¼š")
    generate_button = gr.Button("ç”ŸæˆéŸ³ä¹")
    music_output = gr.Textbox(label="Qwen ç”Ÿæˆçš„éŸ³ä¹æè¿°")
    audio_output = gr.Audio(label="ğŸ§ æ’­æ”¾éŸ³ä¹", type="filepath")
    generate_button.click(gradio_generate, inputs=[scene_input], outputs=[music_output, audio_output])
demo.launch(share=True)
