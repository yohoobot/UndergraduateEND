# ä»…é¦–æ¬¡è¿è¡Œ
!pip install gradio
!pip install transformers torchaudio audiocraft accelerate bitsandbytes

#1 qwen-éƒ¨ç½²musicgen-small(colab)
import json
import random
import torch
import requests
import torchaudio
import gradio as gr
from transformers import MusicgenProcessor, MusicgenForConditionalGeneration

# Qwen API é…ç½®
QWEN_API_KEY = "sk-"  # é˜¿é‡Œäº‘DashScope Key (qwen2.5)
QWEN_API_URL = "https://dashscope.aliyuncs.com/api/v1/services/aigc/text-generation/generation"

# åŠ è½½æ•°æ®ï¼ˆJSONLï¼‰ï¼Œæ ¼å¼ä¸º {"scene": "...", "music": "..."}
scene_music_data = []
with open("musicgen_scene_music_pairs.jsonl", "r", encoding="utf-8") as f:
    for line in f:
        scene_music_data.append(json.loads(line.strip()))

# åŠ è½½ MusicGen æ¨¡å‹
MODEL_NAME = "facebook/musicgen-small" # ç®—åŠ›å……è¶³å¯æ›¿æ¢ä¸ºmedium/large
processor = MusicgenProcessor.from_pretrained(MODEL_NAME)
model = MusicgenForConditionalGeneration.from_pretrained(MODEL_NAME)
model = model.to("cuda" if torch.cuda.is_available() else "cpu")

# ==========================few-shot===================================
# Step 1: æ„é€ å¤šè½®å¯¹è¯æ ¼å¼çš„ç¤ºä¾‹ kä¸ºæ¯è½®å¯¹è¯ä½¿ç”¨çš„ç¤ºä¾‹æ•°é‡ï¼Œé»˜è®¤=2
def build_few_shot_messages(k=2):
    examples = random.sample(scene_music_data, k)
    messages = [{"role": "system", "content": "You are a music cognition expert converting restaurant scene descriptions into music prompts suitable for MusicGen."}]
    for ex in examples:
        messages.append({"role": "user", "content": f"Scene: {ex['scene']}"})
        messages.append({"role": "assistant", "content": ex['music']})
    return messages

# Step 2: è®­ç»ƒåæµ‹è¯•
def generate_music_description(scene_desc):
    messages = build_few_shot_messages(k=2)
    messages.append({"role": "user", "content": f"Scene: {scene_desc}"})

    # è¯·æ±‚å¤´æ„å»º
    headers = {
        "Authorization": f"Bearer {QWEN_API_KEY}",
        "Content-Type": "application/json"
    }
    payload = {
        "model": "qwen2.5-14b-instruct",
        "input": {"messages": messages},
        "parameters": {"temperature": 0.5, "max_tokens": 200}
    }
    response = requests.post(QWEN_API_URL, headers=headers, json=payload)
    if response.status_code == 200:
        result = response.json()
        return result.get("output", {}).get("text", "No output")
    else:
        return f"Error: {response.text}"

# ==========================éŸ³ä¹ç”Ÿæˆ===================================
# Step 3: ç”ŸæˆéŸ³ä¹+é‡Šæ”¾æ˜¾å­˜
def generate_music_from_text(music_desc, duration=12):
    inputs = processor(text=[music_desc], padding=True, return_tensors="pt").to(model.device)
    with torch.no_grad():
        outputs = model.generate(**inputs, max_length=600, do_sample=False)
    waveform = torch.tensor(outputs[0].cpu())
    sample_rate = 32000
    audio_path = "generated_music.wav"
    torchaudio.save(audio_path, waveform, sample_rate)

    del inputs, outputs, waveform
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
    return audio_path

# Step 4: Gradioé¡µé¢åˆ›å»º
# å‰ç«¯
def gradio_generate(scene_description):
    music_desc = generate_music_description(scene_description)
    if "Error" in music_desc:
        return music_desc, None
    audio_path = generate_music_from_text(music_desc)
    return music_desc, audio_path
# Gradio UI
with gr.Blocks() as demo:
    gr.Markdown("## èƒŒæ™¯éŸ³ä¹ç”Ÿæˆå™¨")
    scene_input = gr.Textbox(label="é¤é¦†ç¯å¢ƒæè¿°", placeholder="å¦‚ï¼šæ¸©é¦¨çš„æ„å¤§åˆ©é¤å…ï¼Œé€‚åˆæƒ…ä¾£çº¦ä¼š")
    generate_button = gr.Button("ç”ŸæˆéŸ³ä¹")
    music_output = gr.Textbox(label="ç”Ÿæˆçš„éŸ³ä¹æè¿°")
    audio_output = gr.Audio(label="ğŸ§ æ’­æ”¾éŸ³ä¹", type="filepath")
    generate_button.click(gradio_generate, inputs=[scene_input], outputs=[music_output, audio_output])
demo.launch(share=True)
